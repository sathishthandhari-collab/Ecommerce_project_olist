# Olist E-commerce Analytics Engineering Project

## ğŸ¯ Project Overview

A comprehensive **end-to-end Analytics Engineering solution** for Olist's Brazilian e-commerce marketplace data, implementing enterprise-grade data transformation patterns using **dbt Core**, **Snowflake**, **Apache Airflow**, and **GitHub Actions**. This project demonstrates advanced analytical capabilities including customer lifetime value prediction, fraud detection, executive KPIs, and **production-ready orchestration**.

## Project Navigation
```
Ecommerce_Project_Olist
â”œâ”€â”€ .github
â”‚   â””â”€ workflows                                  ## Github action workflows
â”œâ”€â”€ Airflow
â”‚   â”œâ”€â”€ Dev, Prod, Monitoring dag scripts         ## Airflow Dags
â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â””â”€â”€ Olist_DBT_Transformation              ## DBT PROJECT
â”‚   â”‚       â”œâ”€â”€ models
â”‚   â”‚       â”‚   â”œâ”€â”€ staging
â”‚   â”‚       â”‚   â”œâ”€â”€ intermediate
â”‚   â”‚       â”‚   â””â”€â”€ marts
â”‚   â”‚       â”œâ”€â”€ macros
â”‚   â”‚       â”œâ”€â”€ snapshots
â”‚   â”‚       â”œâ”€â”€ tests
â”‚   â”‚       â””â”€â”€ project.yml
â”‚   â”‚
â”‚   â”œâ”€ docker-compose.yaml
â”‚   â”œâ”€ Dockerfile
â”‚   â””â”€ Requirement.txt
â”œâ”€â”€ LOAD
â”œâ”€â”€ Readme.md
â””â”€â”€ Visuals
```

## ğŸ—ï¸ Architecture & Data Lineage

### Complete Data Pipeline Architecture
```
Daily Batch upload to S3
    â†“
Raw Data Loaded to Snowflake tables via snowpipe
    â†“
Transformed With DBT
    â†“
Staging Layer (8 models) - Data Quality & Standardization
    â†“  
Incremental Intermediate Layer (5 models) - Business Logic & Analytics
    â†“
Marts Layer (8 models) - Executive & Operational Reporting
    â†“
100+ tests & SCD type-II snapshots
    â†“
Apache Airflow Orchestration (3 Production DAGs)
    â†“
Power BI Dashboards & Analysis
```

### **ğŸš€ Production Airflow Orchestration**

#### **DAG Architecture:**
- **`olist_production_dag.py`** â†’ Daily production pipeline with comprehensive error handling
- **`olist_development_dag.py`** â†’ Development environment testing and validation  
- **`olist_monitoring_dag.py`** â†’ Data quality monitoring and cost optimization alerts

#### **Production DAG Features:**
- **Automated Daily Runs** at 7:00 AM UTC with 3-retry logic
- **Dynamic Task Generation** based on dbt model dependencies
- **Smart Failure Recovery** with Slack notifications and partial refresh capabilities
- **Cost Monitoring Integration** with Snowflake credit usage tracking
- **Data Quality Validation** with automated test execution and threshold alerts
- **Environment-Specific Configurations** (dev sampling vs production full refresh)

#### **Monitoring & Alerting:**
- **Real-time Slack alerts** for pipeline failures and data quality issues
- **Automatic cost monitoring** with budget threshold notifications
- **Data freshness monitoring** across all mart tables
- **Performance tracking** with query execution time alerts
- **Custom SLA monitoring** for executive dashboard availability

### Detailed Data Lineage

**Staging Layer:**
- `stg_customers.sql` â†’ Customer demographic data with location standardization
- `stg_orders.sql` â†’ Order lifecycle and delivery tracking
- `stg_order_items.sql` â†’ Product-level order details 
- `stg_payments.sql` â†’ Payment method and installment analysis
- `stg_products.sql` â†’ Product catalog with categorization
- `stg_sellers.sql` â†’ Seller geographic and performance data
- `stg_reviews.sql` â†’ Customer satisfaction and sentiment analysis
- `stg_geolocation.sql` â†’ Brazilian geographic reference data

**Intermediate Layer:**
- `int_customer_360.sql` â†’ Complete customer behavior profile (RFM analysis, lifecycle scoring)
- `int_customer_lifetime_value.sql` â†’ CLV prediction with confidence intervals
- `int_order_anomalies.sql` â†’ Statistical anomaly detection using Z-scores  
- `int_seller_health_score.sql` â†’ Multi-dimensional seller performance scoring
- `int_sales_analysis.sql` â†’ - Multi-dimensional sales analysis (monthly/state/category/region)

**Marts Layer:**
- `mart_executive_kpis.sql` â†’ C-suite dashboards with growth metrics
- `mart_financial_performance.sql` â†’ Revenue analysis with forecasting
- `mart_customer_strategy.sql` â†’ Customer segmentation and retention analytics
- `mart_seller_management.sql` â†’ Seller performance and health monitoring
- `mart_logistics_performance.sql` â†’ Delivery and fulfillment analytics
- `mart_fraud_monitoring.sql` â†’ Real-time fraud alerts and risk scoring
- `mart_payment_method_performance.sql` â†’ Payment channel optimization
- `mart_unit_economics.sql` â†’ Cohort analysis and unit profitability

## **Tests & snapshots**
- 100+ dbt tests covering data quality, business logic, referential integrity
- SCD Type-II snapshots for customer and seller dimension history tracking
- Contracts enforced to ensure No compatibility issues


## ğŸš€ Key Features

### **Production Orchestration**
- **Apache Airflow Integration** with production-ready DAGs and monitoring
- **Automated Daily Pipeline** with intelligent retry logic and failure recovery
- **Multi-Environment Management** (development, staging, production)
- **Real-time Monitoring** with Slack integration and cost tracking
- **SLA Management** with automated alerting and escalation

### Advanced Analytics
- **Customer Lifetime Value (CLV)** prediction with 95% confidence intervals
- **Churn probability** modeling using logistic regression
- **Anomaly detection** with configurable Z-score thresholds  
- **Cohort analysis** for retention metrics
- **RFM segmentation** for customer targeting
- **Multi-dimensional sales analysis** across time/geography/products

### Data Quality & Governance
- **Automated data quality scoring** with configurable thresholds
- **Contract enforcement** for schema stability  
- **Advanced testing** with custom expectations
- **Cost monitoring** and warehouse optimization
- **Incremental processing** with smart refresh logic

### Performance Optimization
- **Incremental materialization** with merge strategies
- **Clustering** on high-cardinality dimensions
- **Development vs Production** configurations
- **Sampling** for faster development iterations
- **Query cost tracking** and optimization

## ğŸ› ï¸ Technical Implementation

### **Airflow Configuration Highlights**
- **Production-Grade DAGs** with comprehensive error handling and recovery
- **Dynamic Task Generation** based on dbt model lineage
- **Cost Optimization Logic** with automatic warehouse scaling
- **Multi-Environment Support** with environment-specific configurations
- **Advanced Monitoring** with custom operators and sensors
<!-- - **Slack Integration** for real-time operational alerts -->

### **Airflow DAG Features:**
```python
# Production DAG Capabilities
â”œâ”€â”€ Automated daily scheduling (7:00 AM UTC)
â”œâ”€â”€ Dynamic dbt task generation 
â”œâ”€â”€ Comprehensive error handling (3 retries)
â”œâ”€â”€ Cost monitoring and budget alerts
â”œâ”€â”€ Data quality validation with custom tests
â”œâ”€â”€ Slack notifications for failures
â”œâ”€â”€ Environment-specific variable management
â”œâ”€â”€ SLA monitoring with executive dashboard priority
â””â”€â”€ Automatic documentation generation
```

### dbt Configuration Highlights
- **Multi-environment setup**: Dev (sampled) vs Prod (full data)
- **Advanced materialization strategies**: Incremental, table, view
- **Custom schema organization**: Staging, intermediate, marts
- **Automated testing**: Data quality, business logic, referential integrity
- **Snapshots**: Implemented SCD TYPE-II snapshots to monitor schema changes
- **Cost monitoring**: Credits usage and performance tracking
- **Custom Macros**: Created macros that can get reccuring calculations adhering DRI practice

### Key Macros & Utilities
- `calculate_z_score()` - Statistical anomaly detection
- `data_quality_score()` - Automated quality scoring  
- `capture_run_costs()` - Snowflake cost monitoring
- `standardize_timestamp()` - Timezone standardization
- Incremental helpers for efficient processing

### **Complete CI/CD Pipeline**
```yaml
End-to-End Deployment Pipeline:
â”œâ”€â”€ GitHub Actions (Code Quality & Testing)
â”œâ”€â”€ dbt compilation and testing (dev/prod)  
â”œâ”€â”€ Airflow DAG deployment to production
â”œâ”€â”€ Automated testing with data validation
â”œâ”€â”€ Production monitoring activation
â”œâ”€â”€ Power BI dashboard refresh integration
â””â”€â”€ Cost monitoring and alerting setup
```

## ğŸ“Š Business Impact & Use Cases

### **Executive Reporting (Automated via Airflow)**
- **Daily Executive KPI Refresh** with automated email reports
- **Monthly/Weekly KPIs** with automated alerting
- **Revenue forecasting** using moving averages and seasonal indexing
- **Market penetration analysis** across Brazilian states
- **Platform health monitoring** with anomaly detection

### **Operational Analytics (Real-time Monitoring)**
- **Real-time fraud detection** with severity scoring and immediate alerts
- **Seller performance management** with health tiers and intervention flags
- **Logistics optimization** with delivery performance tracking  
- **Payment method analysis** with conversion optimization

### **Strategic Insights (Automated Delivery)**
- **Customer segmentation** (Champions, Loyalists, At-Risk, Lost) with campaign triggers
- **Product performance** across categories and regions
- **Market expansion** opportunities identification
- **Revenue optimization** through unit economics analysis

## **ğŸ“Š Production Power BI Integration**

### **Executive Dashboard Suite (Auto-Refreshing)**
- **Executive Performance Dashboard** - C-level KPIs with real-time alerts
- **Operations Intelligence Dashboard** - Seller health and logistics monitoring
- **Customer Strategy Dashboard** - CLV optimization and churn prevention

### **Power BI Integration Features:**
- **Automated Daily Refresh** triggered by Airflow DAG completion
- **Real-time Data Connection** to Snowflake marts
- **Executive Alert System** with email notifications
- **Mobile-Responsive Design** for executive access
- **Advanced DAX Calculations** leveraging dbt-computed metrics

## ğŸƒâ€â™‚ï¸ Getting Started

### Prerequisites
```bash
# Install required packages
pip install dbt-core==1.10.13 dbt-snowflake==1.10.2
pip install apache-airflow==2.7.0 apache-airflow-providers-snowflake

# Clone repository
git clone <repository-url>
cd olist_dbt_transformation
```

### **Airflow Setup**
```bash
# Initialize Airflow
export AIRFLOW_HOME=~/airflow
airflow db init

# Create admin user
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# Copy DAG files to Airflow
cp dags/*.py $AIRFLOW_HOME/dags/

# Configure connections (Snowflake, Slack)
airflow connections add snowflake_default \
    --conn-type snowflake \
    --conn-host <your-account>.snowflakecomputing.com \
    --conn-login <username> \
    --conn-password <password> \
    --conn-schema <database> \
    --conn-extra '{"warehouse": "<warehouse>", "database": "<database>", "role": "<role>"}'

# Start Airflow services
airflow webserver --port 8080 &
airflow scheduler &
```

### Environment Setup
```bash
# Install dependencies
dbt deps --profiles-dir . --project-dir .

# Test connection
dbt debug --profiles-dir . --project-dir .

# Development run (sampled data)
dbt run --target dev --profiles-dir . --project-dir .

# Production run (full data) - typically handled by Airflow
dbt run --target prod --profiles-dir . --project-dir .
```

### **Key Commands**

#### **dbt Commands:**
```bash
# Full refresh with quality tests
dbt run --full-refresh && dbt test

# Specific model execution
dbt run --select mart_executive_kpis+

# Cost monitoring
dbt run --vars '{capture_costs: true}'

# Documentation generation
dbt docs generate && dbt docs serve
```

#### **Airflow Commands:**
```bash
# Test DAG execution
airflow dags test olist_production_dag 2024-01-01

# Trigger manual DAG run
airflow dags trigger olist_production_dag

# Monitor DAG status
airflow dags list
airflow tasks list olist_production_dag

# View logs
airflow logs <dag_id> <task_id> <execution_date>
```

## ğŸ“ˆ Data Models Documentation

### **Sales Analytics Models (NEW)**
- **int_sales_analysis**: Comprehensive multi-dimensional sales analysis
  - Monthly performance tracking across states and product categories
  - Regional trade pattern analysis with cross-state commerce insights
  - Customer behavior analytics with engagement tiers
  - Growth rate calculations with month-over-month comparisons

### Customer Analytics Models
- **int_customer_360**: 360-degree customer view with behavioral scoring
- **mart_customer_strategy**: Strategic customer insights for retention/acquisition
- **int_customer_lifetime_value**: Predictive CLV modeling with confidence bands

### Financial & Revenue Models  
- **mart_financial_performance**: Comprehensive revenue analysis with forecasting
- **mart_unit_economics**: Cohort-based profitability analysis
- **mart_payment_method_performance**: Payment channel optimization insights

### Operational Models
- **mart_fraud_monitoring**: Real-time fraud detection and alerting
- **mart_logistics_performance**: End-to-end delivery performance tracking
- **mart_seller_management**: Seller ecosystem health monitoring

## **ğŸ”§ Production Monitoring & Operations**

### **Airflow Monitoring Dashboard**
- **DAG Success/Failure Rates** with historical trending
- **Task Duration Monitoring** with SLA tracking
- **Cost per DAG Run** with budget variance alerts
- **Data Quality Score Tracking** across all mart tables

### **Automated Alerts & Notifications**
```yaml
Alert Configuration:
â”œâ”€â”€ Pipeline Failures â†’ Immediate Slack notification
â”œâ”€â”€ Data Quality Issues â†’ Email to data team
â”œâ”€â”€ Cost Overruns â†’ Finance team notification  
â”œâ”€â”€ SLA Violations â†’ Executive escalation
â”œâ”€â”€ Anomaly Detection â†’ Business team alerts
â””â”€â”€ System Health â†’ Operations dashboard updates
```

### **Performance Optimization**
- **Intelligent Warehouse Scaling** based on workload
- **Query Performance Monitoring** with automatic optimization recommendations
- **Cost-per-Query Tracking** with efficiency metrics
- **Resource Utilization Analysis** for capacity planning

## ğŸ“‹ Business Glossary

### Key Metrics
- **GMV (Gross Merchandise Value)**: Total value of orders placed
- **AOV (Average Order Value)**: Mean order value per transaction
- **CLV (Customer Lifetime Value)**: Predicted total customer value
- **CAC (Customer Acquisition Cost)**: Cost to acquire new customers
- **Churn Rate**: Percentage of customers who stop purchasing

### Customer Segments
- **Champions**: High-value, frequent, recent customers
- **Loyalists**: Consistent, satisfied customers  
- **Potential Loyalists**: Recent customers with growth potential
- **At-Risk**: Previously valuable customers showing decline
- **Lost Customers**: Inactive customers requiring win-back campaigns

## ğŸ¯ Analytics Engineering Best Practices Demonstrated

### **Production Orchestration**
- **Enterprise Airflow Implementation** with production-grade monitoring
- **Multi-environment deployment** with automated testing
- **Cost-conscious orchestration** with budget controls
- **Failure recovery automation** with intelligent retry logic

### Data Modeling
- **Dimensional modeling** with proper fact/dimension separation
- **Slowly Changing Dimensions (SCD)** implementation
- **Surrogate key** management with dbt utilities
- **Referential integrity** maintenance

### Code Quality
- **Modular design** with reusable macros
- **Version control** with GitHub integration
- **Documentation** with business context
- **Testing strategy** covering data quality and business logic

### Performance Engineering
- **Incremental processing** patterns
- **Clustering optimization** for analytical workloads  
- **Query performance monitoring**
- **Cost-conscious development practices**

---

## ğŸ† Project Achievements

This project demonstrates **enterprise-level Analytics Engineering** capabilities:

âœ… **Production-Ready Architecture**: Complete end-to-end pipeline with Airflow orchestration and monitoring

âœ… **Scalable Infrastructure**: Multi-layered approach supporting growth from startup to enterprise scale

âœ… **Advanced Analytics**: Implementation of CLV, churn prediction, anomaly detection, and multi-dimensional sales analysis

âœ… **Operational Excellence**: Automated testing, monitoring, cost optimization, and failure recovery

âœ… **Executive Intelligence**: Production Power BI dashboards with automated refresh and alerting

âœ… **Cost Management**: Comprehensive monitoring and optimization with budget controls

âœ… **Technical Rigor**: Best practices in dbt development, Airflow orchestration, Snowflake optimization, and CI/CD

**Perfect for showcasing Senior/Principal Analytics Engineering expertise in job interviews and demonstrating readiness for technical leadership roles in data-driven organizations requiring production-scale analytics infrastructure.**

## **ğŸš€ Production Deployment Checklist**

### **Infrastructure Setup**
- [ ] Airflow production environment configured
- [ ] Snowflake production warehouse provisioned
- [ ] Slack workspace integration setup
- [ ] Power BI workspace configured with service principal
- [ ] GitHub Actions secrets configured

### **Security & Access**
- [ ] Role-based access control (RBAC) implemented
- [ ] Service account authentication configured
- [ ] Data encryption at rest and in transit verified
- [ ] PII data masking implemented
- [ ] Audit logging enabled

### **Monitoring & Alerting**
- [ ] Airflow monitoring dashboard deployed
- [ ] Cost monitoring thresholds configured
- [ ] Data quality SLAs defined
- [ ] Executive alerting rules activated
- [ ] Performance monitoring enabled

**This comprehensive solution demonstrates production-ready Analytics Engineering at the Principal level, perfect for 20-25 LPA role justification!** ğŸ¯